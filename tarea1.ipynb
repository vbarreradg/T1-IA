{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from huffman import HuffmanCoding\n",
    "from huffmancodec import HuffmanCodec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: noring@netcom.com (Jon Noring)Subject: Good Grief!  (was Re: Candida Albicans: what is it?)Organization: Netcom Online Communications Services (408-241-9760 login: guest)Lines: 81In article rind@enterprise.bih.harvard.edu (David Rind) writes:>In article davpa@ida.liu.se  (David Partain) writes:>>Someone I know has recently been diagnosed as having Candida Albicans, >>a disease about which I can find no information.  Apparently it has something>>to do with the body's production of yeast while at the same time being highly>>allergic to yeast.  Can anyone out there tell me any more about it?>Candida albicans can cause severe life-threatening infections, usually>in people who are otherwise quite ill.  This is not, however, the sort>of illness that you are probably discussing.>>\"Systemic yeast syndrome\" where the body is allergic to>yeast is considered a quack diagnosis by mainstream medicine.  There>is a book \"The Yeast Connection\" which talks about this \"illness\".>>There is no convincing evidence that such a disease exists.There's a lot of evidence, it just hasn't been adequately gathered andpublished in a way that will convince the die-hard melancholic skepticswho quiver everytime the word 'anecdote' or 'empirical' is used.For example, Dr. Ivker, who wrote the book \"Sinus Survival\", always gives,before any other treatment, a systemic anti-fungal (such as Nizoral) to hisnew patients IF they've been on braod-spectrum anti-biotics 4 or more timesin the last two years.  He's kept a record of the results, and for over 2000 patients found that over 90% of his patients get significant reliefof allergic/sinus symptoms.  Of course, this is only the beginning for hisprogram.In my case, as I reported a few weeks ago, I was developing the classicsymptoms outlined in 'The Yeast Connection' (I agree it is a poorly written book):  e.g., extreme sensitivity to plastics, vapors, etc. whichI never had before (started in November).  Within one week of full dosageof Sporanox, the sensitivity to chemicals has fully disappeared - I cannow sit on my couch at home without dying after two minutes.  I'm also*greatly* improved in other areas as well.Of course, I have allergy symptoms, etc.  I am especially allergic tomolds, yeasts, etc.  It doesn't take a rocket scientist to figure out thatif one has excessive colonization of yeast in the body, and you have anatural allergy to yeasts, that a threshold would be reached where youwould have perceptible symptoms.  Also, yeast do produce toxins of varioussorts, and again, you don't have to be a rocket scientist to realize thatsuch toxins can cause problems in some people.  In my case it was sinussince that's the center of my allergic response.  Of course, the $60,000question is whether a person who is immune compromised (as tests showed I wasfrom over 5 years of antibiotics, nutritionally-deficiencies because of thestress of infections and allergies, etc.), can develop excessive yeastcolonization somewhere in the body.  It is a tough question to answer sincetesting for excessive yeast colonization is not easy.  One almost has totake an empirical approach to diagnosis.  Fortunately, Sporanox is relativelysafe unlike past anti-fungals (still have to be careful, however) so there'sno reason any longer to withhold Sporanox treatment for empirical reasons.BTW, some would say to try Nystatin.  Unfortunately, most yeast grows hyphaetoo deep into tissue for Nystatin to have any permanent affect.  You'll finda lot of people who are on Nystatin all the time.In summary, I appreciate all of the attempts by those who desire to keepmedicine on the right road.  But methinks that some who hold too firmlyto the party line are academics who haven't been in the trenches long enoughactually treating patients.  If anybody, doctors included, said to me to myface that there is no evidence of the 'yeast connection', I cannot guaranteetheir safety.  For their incompetence, ripping off their lips is justified asfar as I am concerned.Jon Noring-- Charter Member --->>>  INFJ Club.If you're dying to know what INFJ means, be brave, e-mail me, I'll send info.=============================================================================| Jon Noring          | noring@netcom.com        |                          || JKN International   | IP    : 192.100.81.100   | FRED'S GOURMET CHOCOLATE || 1312 Carlton Place  | Phone : (510) 294-8153   | CHIPS - World's Best!    || Livermore, CA 94550 | V-Mail: (510) 417-4101   |                          |=============================================================================Who are you?  Read alt.psychology.personality!  That's where the action is.\n"
     ]
    }
   ],
   "source": [
    "# obtengo los nombres de los archivos en una carpeta\n",
    "folder = 'sci.med'\n",
    "files = [f for f in listdir('./20news-bydate-train/' + folder) if isfile(join('./20news-bydate-train/' + folder, f))]\n",
    "\n",
    "# obtengo una cantidad n de archivos aleatorios\n",
    "numero_archivos = 500\n",
    "random_files = random.sample(files, numero_archivos)\n",
    "text_concatenation = ''\n",
    "for file in random_files:\n",
    "    f=open('./20news-bydate-train/'+ folder + '/' + file, \"r\")\n",
    "    try:\n",
    "        lines = f.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        lines = []\n",
    "    text = ''\n",
    "    for line in lines:\n",
    "        text += line[:-1]\n",
    "    f.close()\n",
    "    text_concatenation += text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creo una muestra aleatoria de ejemplos\n",
    "numero_datos = 30000\n",
    "tamaño_ventana = 5\n",
    "random_indexes = random.sample(range(len(text_concatenation) - tamaño_ventana), numero_datos)\n",
    "random_samples = []\n",
    "for i in random_indexes:\n",
    "    sample = text_concatenation[i:i+tamaño_ventana]\n",
    "    random_samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþ\n",
      "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ', '\\n', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ'])\n"
     ]
    }
   ],
   "source": [
    "# hago un diccionario one hot con todos los caracteres ascii\n",
    "chars = string.printable[:-5]\n",
    "chars += '\\n'\n",
    "for i in range(161, 255):\n",
    "    chars += chr(i)\n",
    "print(chars)\n",
    "one_hot = {}\n",
    "for i, l in enumerate(chars):\n",
    "    vector = np.zeros(190)\n",
    "    vector[i] = 1\n",
    "    one_hot[l] = vector\n",
    "print(one_hot.keys())\n",
    "\n",
    "def window_onehot(window, one_hot):\n",
    "    inp = []\n",
    "    for char in window[:-1]:\n",
    "        try:\n",
    "            inp.append(one_hot[char])\n",
    "        except KeyError:\n",
    "            if (char == '\\t'):\n",
    "                inp.append(one_hot['t'])\n",
    "            elif (char == '\\r'):\n",
    "                inp.append(one_hot['r'])\n",
    "            else:\n",
    "                inp.append(one_hot['\\\\'])\n",
    "    sum_vector = np.zeros(190)\n",
    "    for vector in inp:\n",
    "        sum_vector += vector\n",
    "    return sum_vector\n",
    "\n",
    "# para cada ejemplo de la muestras de ejemplos lo paso a one hot\n",
    "input_samples = []\n",
    "output_samples = []\n",
    "for sample in random_samples:\n",
    "    sum_vector = window_onehot(sample, one_hot)\n",
    "    input_samples.append(sum_vector)\n",
    "    \n",
    "    try:\n",
    "        output_samples.append(one_hot[sample[-1]])\n",
    "    except KeyError:\n",
    "        if (sample[-1] == '\\t'):\n",
    "            output_samples.append(one_hot['t'])\n",
    "        elif (sample[-1] == '\\r'):\n",
    "            output_samples.append(one_hot['r'])\n",
    "        else:\n",
    "            output_samples.append(one_hot['\\\\'])\n",
    "            \n",
    "output_samples = []\n",
    "for sample in random_samples:\n",
    "    #if sample[-1] == ' ':\n",
    "    #output_samples.append(random.choice(chars))\n",
    "    #else:\n",
    "    #   output_samples.append(sample[-1])\n",
    "    output_samples.append(sample[-1])\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#le.fit(input_samples)\n",
    "#input_samples = le.transform(input_samples)\n",
    "#output_samples = le.transform(output_samples)\n",
    "train_inputs = input_samples[:int((len(input_samples)/3)*2)]\n",
    "train_outputs = output_samples[:int((len(input_samples)/3)*2)]\n",
    "test_inputs = input_samples[int((len(input_samples)/3)*2):]\n",
    "test_outputs = output_samples[int((len(input_samples)/3)*2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_performance(classifier, test_inputs, test_outputs):\n",
    "    probs = classifier.predict_proba(test_inputs)\n",
    "    pred = classifier.predict(test_inputs)\n",
    "    print(probs)\n",
    "    print(pred)\n",
    "    matrix = confusion_matrix(test_outputs, pred)\n",
    "    acc = accuracy_score(test_outputs, pred)\n",
    "    #print(matrix)\n",
    "    print('Precisión: ' + str(acc))\n",
    "    return probs\n",
    "\n",
    "def train_model(train_inputs, train_outputs, penalization):\n",
    "    classifier = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "    classifier.fit(train_inputs, train_outputs)\n",
    "    #classifier = SVC(decision_function_shape='ovo', C=penalization, probability=True)\n",
    "    #classifier.fit(train_inputs, train_outputs)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempeño con set de test:\n",
      "[[ 0.          0.          0.01375    ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.89484502 ...,  0.00645166  0.          0.        ]\n",
      " [ 0.          0.          0.27921491 ...,  0.01696491  0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.11216017 ...,  0.01        0.          0.        ]\n",
      " [ 0.          0.          0.80829606 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.09889009 ...,  0.          0.          0.        ]]\n",
      "['s' ' ' ' ' ..., 'e' ' ' 'P']\n",
      "Precisión: 0.3429\n",
      "Desempeño con set de entrenamiento:\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.379221   ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.89484502 ...,  0.00645166  0.          0.        ]\n",
      " [ 0.          0.          0.01285714 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.34180245 ...,  0.          0.          0.        ]]\n",
      "['w' ' ' 'f' ..., ' ' 'g' 'e']\n",
      "Precisión: 0.61995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.379221  , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.89484502, ...,  0.00645166,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01285714, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.34180245, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = train_model(train_inputs, train_outputs, 2)\n",
    "\n",
    "print('Desempeño con set de test:')\n",
    "probs = model_performance(classifier, test_inputs, test_outputs)\n",
    "print('Desempeño con set de entrenamiento:')\n",
    "model_performance(classifier, train_inputs, train_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_U}\n",
      "[[ 0.          0.          0.20667586  0.          0.          0.04        0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.01        0.          0.          0.01        0.          0.          0.\n",
      "   0.          0.          0.          0.00949153  0.0175      0.          0.\n",
      "   0.02306061  0.          0.04        0.01        0.01        0.          0.0325\n",
      "   0.          0.04        0.          0.          0.01        0.          0.02\n",
      "   0.02333333  0.          0.          0.03666667  0.04        0.03        0.\n",
      "   0.          0.          0.06        0.          0.          0.          0.\n",
      "   0.          0.01        0.13677201  0.          0.01        0.          0.\n",
      "   0.01        0.04        0.          0.          0.          0.05        0.\n",
      "   0.          0.          0.          0.044       0.          0.          0.\n",
      "   0.01        0.          0.02        0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#initial_window = np.zeros(189)\n",
    "#for i in range(7):\n",
    "#    random_index = random.randint(0, 189)\n",
    "#    initial_window[random_index] += 1\n",
    "#print(initial_window)\n",
    "#probs = classifier.predict_proba([initial_window])\n",
    "classes = classifier.classes_\n",
    "initial_window = ''\n",
    "for i in range(tamaño_ventana - 1):\n",
    "    initial_window += random.choice(string.printable)\n",
    "print(initial_window)\n",
    "initial_onehot = window_onehot(initial_window, one_hot)\n",
    "\n",
    "probs = classifier.predict_proba([initial_onehot])\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_freq_dic():\n",
    "    dic = {}\n",
    "    for c in chars:\n",
    "        dic[c] = 0\n",
    "    return dic\n",
    "\n",
    "def probs_to_freq(probs, dic, classifier):\n",
    "    classes = classifier.classes_\n",
    "    classes_list = list(classes)\n",
    "    for c, value in enumerate(classes_list):\n",
    "        dic[value] = int(probs[0][c]*1000)\n",
    "    return dic\n",
    "\n",
    "def get_probs(window, classifier, one_hot):\n",
    "    window_one_hot = window_onehot(window, one_hot)\n",
    "    probs = classifier.predict_proba([window_one_hot])\n",
    "    return probs\n",
    "\n",
    "def get_codes(freqs, file):\n",
    "    #codec = HuffmanCodec.from_frequencies(freqs)\n",
    "    #return codec.get_code_table()\n",
    "    h = HuffmanCoding(file)\n",
    "    codes, reverse_codes = h.get_codes(freqs)\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./20news-bydate-train/sci.med/59051\n",
      "M_U}From: nyeda@cnsvax.uwec.edu (David Nye)\n",
      "Subject: Re: seizures ( infantile spasms )\n",
      "Organization: University of Wisconsin Eau Claire\n",
      "Lines: 19\n",
      "\n",
      "[reply to dufault@lftfld.enet.dec.com (MD)]\n",
      " \n",
      ">After many metabolic tests, body structure tests, and infection/virus\n",
      ">tests the doctors still do not know quite what type of siezures he is\n",
      ">having (although they do have alot of evidence that it is now pointing\n",
      ">to infantile spasms ).  This is where we stand right now....As I know\n",
      ">now, these particular types of disorders are still not really well\n",
      ">understood by the medical community.\n",
      " \n",
      "Infantile spasms have been well understood for quite some time now.  You\n",
      "are seeing a pediatric neurologist, aren't you?  If not, I strongly\n",
      "recommend it.  There is a new anticonvulsant about to be released called\n",
      "felbamate which may be particularly helpful for infantile spasms.  As\n",
      "for learning more about seizures, ask your doctor or his nurse about a\n",
      "local support group.\n",
      " \n",
      "David Nye (nyeda@cnsvax.uwec.edu).  Midelfort Clinic, Eau Claire WI\n",
      "This is patently absurd; but whoever wishes to become a philosopher\n",
      "must learn not to be frightened by absurdities. -- Bertrand Russell\n",
      "\n",
      "9426\n"
     ]
    }
   ],
   "source": [
    "# obtengo los nombres de los archivos en una carpeta\n",
    "folder = 'sci.med'\n",
    "files = [f for f in listdir('./20news-bydate-train/' + folder) if isfile(join('./20news-bydate-train/' + folder, f))]\n",
    "\n",
    "file = random.sample(files, 1)\n",
    "file_name = './20news-bydate-train/'+ folder + '/' + file[0]\n",
    "print(file_name)\n",
    "\n",
    "#función obtenida de http://bhrigu.me/blog/2017/01/17/huffman-coding-python-implementation/\n",
    "def pad_encoded_text(encoded_text):\n",
    "\t\textra_padding = 8 - len(encoded_text) % 8\n",
    "\t\tfor i in range(extra_padding):\n",
    "\t\t\tencoded_text += \"0\"\n",
    "\n",
    "\t\tpadded_info = \"{0:08b}\".format(extra_padding)\n",
    "\t\tencoded_text = padded_info + encoded_text\n",
    "\t\treturn encoded_text\n",
    "    \n",
    "#función obtenida de http://bhrigu.me/blog/2017/01/17/huffman-coding-python-implementation/\n",
    "def get_byte_array(padded_encoded_text):\n",
    "\t\tif(len(padded_encoded_text) % 8 != 0):\n",
    "\t\t\tprint(\"Encoded text not padded properly\")\n",
    "\t\t\texit(0)\n",
    "\n",
    "\t\tb = bytearray()\n",
    "\t\tfor i in range(0, len(padded_encoded_text), 8):\n",
    "\t\t\tbyte = padded_encoded_text[i:i+8]\n",
    "\t\t\tb.append(int(byte, 2))\n",
    "\t\treturn b\n",
    "\n",
    "def to_compress_file(file_name, initial_window):\n",
    "    text_to_compress = initial_window\n",
    "    f=open(file_name, \"r\")\n",
    "    try:\n",
    "        lines = f.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        lines = []\n",
    "    text = ''\n",
    "    for line in lines:\n",
    "        text += line\n",
    "    f.close()\n",
    "    text_to_compress += text\n",
    "    print(text_to_compress)\n",
    "    return text_to_compress\n",
    "\n",
    "def process_window(window, classifier, one_hot, char_dictionary, file):\n",
    "    prefijo = window[:-1]\n",
    "    charac = window[-1]\n",
    "    probs = get_probs(prefijo, classifier, one_hot)\n",
    "    freqs = probs_to_freq(probs, char_dictionary, classifier)\n",
    "    codes = get_codes(freqs, file)\n",
    "    code = codes[charac]\n",
    "    return code\n",
    "\n",
    "def compress(file, classifier, one_hot, initial_window, tamaño_ventana, output_path):\n",
    "    # creo el texto que debo comprimir, con la ventana inicial\n",
    "    text_to_compress = to_compress_file(file, initial_window)\n",
    "    char_dictionary = create_freq_dic()\n",
    "    \n",
    "    #para cada ventana, generar el código que corresponde a la ventana \n",
    "    encoded_text = ''\n",
    "    for i in range((len(text_to_compress) - tamaño_ventana)):\n",
    "        window = text_to_compress[i:i+tamaño_ventana]\n",
    "        code = process_window(window, classifier, one_hot, char_dictionary, file)\n",
    "        encoded_text += code\n",
    "    print(len(encoded_text))\n",
    "    padded_encoded_text = pad_encoded_text(encoded_text)\n",
    "\n",
    "    b = get_byte_array(padded_encoded_text)\n",
    "    with open(output_path, 'wb') as output:\n",
    "        output.write(bytes(b))\n",
    "    output.close()    \n",
    "        \n",
    "compress(file_name, classifier, one_hot, initial_window, tamaño_ventana, 'compressed.bin')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: nyeda@cnsvax.uwec.edu (David Nye)\n",
      "Subject: Re: seizures ( infantile spasms )\n",
      "Organization: University of Wisconsin Eau Claire\n",
      "Lines: 19\n",
      "\n",
      "[reply to dufault@lftfld.enet.dec.com (MD)]\n",
      " \n",
      ">After many metabolic tests, body structure tests, and infection/virus\n",
      ">tests the doctors still do not know quite what type of siezures he is\n",
      ">having (although they do have alot of evidence that it is now pointing\n",
      ">to infantile spasms ).  This is where we stand right now....As I know\n",
      ">now, these particular types of disorders are still not really well\n",
      ">understood by the medical community.\n",
      " \n",
      "Infantile spasms have been well understood for quite some time now.  You\n",
      "are seeing a pediatric neurologist, aren't you?  If not, I strongly\n",
      "recommend it.  There is a new anticonvulsant about to be released called\n",
      "felbamate which may be particularly helpful for infantile spasms.  As\n",
      "for learning more about seizures, ask your doctor or his nurse about a\n",
      "local support group.\n",
      " \n",
      "David Nye (nyeda@cnsvax.uwec.edu).  Midelfort Clinic, Eau Claire WI\n",
      "This is patently absurd; but whoever wishes to become a philosopher\n",
      "must learn not to be frightened by absurdities. -- Bertrand Russell\n"
     ]
    }
   ],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        bit_string = \"\"\n",
    "        byte = file.read(1)\n",
    "        while(len(byte) > 0):\n",
    "            byte = ord(byte)\n",
    "            bits = bin(byte)[2:].rjust(8, '0')\n",
    "            bit_string += bits\n",
    "            byte = file.read(1)\n",
    "    encoded_text = remove_padding(bit_string)\n",
    "    return encoded_text\n",
    "\n",
    "            \n",
    "def decode_text(encoded_text, reverse_coding):\n",
    "    current_code = \"\"\n",
    "    decoded_text = \"\"\n",
    "\n",
    "    for bit in encoded_text:\n",
    "        current_code += bit\n",
    "        if(current_code in reverse_coding):\n",
    "            character = reverse_coding[current_code]\n",
    "            decoded_text += character\n",
    "            current_code = \"\"\n",
    "\n",
    "    return decoded_text\n",
    "    \n",
    "def remove_padding(padded_encoded_text):\n",
    "    padded_info = padded_encoded_text[:8]\n",
    "    extra_padding = int(padded_info, 2)\n",
    "\n",
    "    padded_encoded_text = padded_encoded_text[8:] \n",
    "    encoded_text = padded_encoded_text[:-1*extra_padding]\n",
    "\n",
    "    return encoded_text\n",
    "\n",
    "def get_reverse_codes(freqs, file):\n",
    "    #codec = HuffmanCodec.from_frequencies(freqs)\n",
    "    #return codec.get_code_table()\n",
    "    h = HuffmanCoding(file)\n",
    "    codes, reverse_codes = h.get_codes(freqs)\n",
    "    return reverse_codes\n",
    "\n",
    "def deprocess_window(window, classifier, one_hot, char_dictionary, file, encoded_text):\n",
    "    prefijo = window\n",
    "    probs = get_probs(prefijo, classifier, one_hot)\n",
    "    freqs = probs_to_freq(probs, char_dictionary, classifier)\n",
    "    reverse_codes = get_reverse_codes(freqs, file)\n",
    "    \n",
    "    current_code = \"\"\n",
    "    for bit in encoded_text:\n",
    "        current_code += bit\n",
    "        if(current_code in reverse_codes):\n",
    "            character = reverse_codes[current_code]\n",
    "            break\n",
    "        else:\n",
    "            character = ''\n",
    "\n",
    "    return character, len(current_code)\n",
    "        \n",
    "\n",
    "def decompress(initial_window, path, classifier, one_hot):\n",
    "    encoded_text = read_file(path)\n",
    "    char_dictionary = create_freq_dic()\n",
    "    \n",
    "    window = initial_window\n",
    "    decoded_text = ''\n",
    "    while len(encoded_text) > 0:\n",
    "        charac, len_code = deprocess_window(window, classifier, one_hot, char_dictionary, path, encoded_text)\n",
    "        decoded_text += charac\n",
    "        encoded_text = encoded_text[len_code:]\n",
    "        window = window[1:] + charac\n",
    "    print(decoded_text)\n",
    "\n",
    "\n",
    "decompress(initial_window,'compressed.bin', classifier, one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
